{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import ASCII text of the books \"Alice in Wonderland\" (downloaded from project Gutenberg) and convert all characters to lowercase so that the model does not treat capitalized words as new words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text and convert to lowercase\n",
    "filename = \"./input/wonderland.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that the Neural Network can use the input, each unique character is mapped to an integer value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapping of unique charst to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c,i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary Statistics of the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  144407\n",
      "Total distinct characters:  45\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", str(n_chars))\n",
    "print(\"Total distinct characters: \", str(n_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A training pattern are the firtst 100 characters, the ground-truth \"label\" for this first training pattern then is the 101st character. This window of 100 characters then gets slided character by character.\n",
    "\n",
    "For illustration purposes assume we would take a sequence of 5 characters, then the first training sample would be `chapt` -> `e` and the second training sample would be `hapte` -> `r`. \n",
    "\n",
    "The characters get converted to integers using the lookup dictionary created before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # of Patterns: 144307\n"
     ]
    }
   ],
   "source": [
    "# Prepare dataset\n",
    "seq_length = 100\n",
    "trainX = []\n",
    "trainY = []\n",
    "for i in range(0, n_chars-seq_length, 1):\n",
    "    seq_in = raw_text[i : i + seq_length]   # in 1st iteration contains first 100 chars\n",
    "    seq_out = raw_text[i + seq_length]   # in 1st iteration contains 101st char\n",
    "    trainX.append([char_to_int[char] for char in seq_in])   # char is the character as string, char_to_int[char] gives the int value\n",
    "    trainX\n",
    "    trainY.append(char_to_int[seq_out])\n",
    "n_patterns = len(trainX)\n",
    "print(\"Total # of Patterns: \" + str(n_patterns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the network we need to transform the data further. \n",
    "1. Reshape the training data to the form [samples, time steps, features].\n",
    "2. Rescale the integers to the range 0-1 to better train the network if a sigmoid function is used (stay in approx linear part of the sigmoid). \n",
    "3. One-hot encode trainY so that each character in y is represented by a vector of 45 (number of distinct characters) values. The character \"n\" (inter value 32) is then represented by a vector of zeros except for one \"1\" in column 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to [samples, time steps, features]\n",
    "X = np.reshape(trainX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one-hot encode y\n",
    "y = np_utils.to_categorical(trainY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I define a LSTM model with one hidden layer and 256 memory units. Dropout is used with a probability of 20%. The output layer is a Dense layer using the softmax activation function. This outputs a probability prediction for each character. These are the parameters used in the tutorial. They are not very well tuned but merely a starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape = (X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation = \"softmax\"))\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I save the model weights after each epoch so that I can use the weights that produced the model with the lowest error for prediction afterwards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define checkpoints\n",
    "filepath = \"./output/weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor = \"loss\", verbose = 1, save_best_only = True, mode = \"min\") \n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "144256/144307 [============================>.] - ETA: 0s - loss: 2.9606Epoch 00001: loss improved from inf to 2.96058, saving model to weights-improvement-01-2.9606.hdf5\n",
      "144307/144307 [==============================] - 1103s 8ms/step - loss: 2.9606\n",
      "Epoch 2/4\n",
      "144256/144307 [============================>.] - ETA: 0s - loss: 2.7602Epoch 00002: loss improved from 2.96058 to 2.76026, saving model to weights-improvement-02-2.7603.hdf5\n",
      "144307/144307 [==============================] - 1014s 7ms/step - loss: 2.7603\n",
      "Epoch 3/4\n",
      "144256/144307 [============================>.] - ETA: 0s - loss: 2.6666Epoch 00003: loss improved from 2.76026 to 2.66660, saving model to weights-improvement-03-2.6666.hdf5\n",
      "144307/144307 [==============================] - 883s 6ms/step - loss: 2.6666\n",
      "Epoch 4/4\n",
      "144256/144307 [============================>.] - ETA: 2s - loss: 2.5864Epoch 00004: loss improved from 2.66660 to 2.58645, saving model to weights-improvement-04-2.5865.hdf5\n",
      "144307/144307 [==============================] - 5896s 41ms/step - loss: 2.5865\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2cdca90a470>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs = 4, batch_size = 128, callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Text with the saved weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load weights\n",
    "filename = \"weights-improvement-04-2.5865.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a reverse mapping from integer to character to convert the predicted integers to characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: \n",
      "\" put on his spectacles and looked anxiously round, to make out who\n",
      "was talking.\n",
      "\n",
      "alice could see, as  \"\n"
     ]
    }
   ],
   "source": [
    "# return a random integer between 0 and the number of different patterns in the training data\n",
    "start = np.random.randint(0, len(trainX)-1)\n",
    "# pick the random pattern\n",
    "pattern = trainX[start]\n",
    "print(\"Seed: \")\n",
    "# print the random pattern by converting the integers to characater\n",
    "print(\"\\\"\", \"\".join([int_to_char[value] for value in pattern]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing 1000 characters of generated text starting with the random pattern above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the soree to the toete the wooee  and the wouee the woeee th the woete the woeee th the woete the woeee th the woete the woeee th the woete the woeee th the woete the woeee th the woete the woeee th the woete the woeee th the woete the woeee th the woete the woeee th the woete the woeee th the woete the woeee th the woete the woeee th the woete the woeee th the woete the woeee th the woete the woeee th the woete the woeee th the woete the woeee th the woete the woeee th the woete the woeee th the woete the woeee th the woete the woeee th the woete the woeee th the woete the woeee th the woete the woeee th the woete the woeee th the woete the woeee th the woete the woeee th the woete the woeee th the woete the woeee th the woete the woeee th the woete the woeee th the woete the woeee th the woete the woeee th the woete the woeee th the woete the woeee th the woete the woeee th the woete the woeee th the woete the woeee th the woete the woeee th the woete the woeee th the woete the woee"
     ]
    }
   ],
   "source": [
    "# initialize empty list for the result_output\n",
    "result_output = []\n",
    "for i in range(1000):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    # prediction contains the probability for each character (0-45) for the given input pattern x\n",
    "    prediction = model.predict(x, verbose = 0)\n",
    "    # index contains the index where the prediction is highest\n",
    "    index = np.argmax(prediction)\n",
    "    # the predicted character\n",
    "    result = int_to_char[index]\n",
    "    # the input sequence \n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    # write to console\n",
    "    sys.stdout.write(result)\n",
    "    # append predicted index to the result_output list\n",
    "    result_output.append(index)\n",
    "    # append predicted index to the pattern\n",
    "    pattern.append(index)\n",
    "    # new pattern is the old pattern with the first character cut away and the new prediction appended to the end. this new pattern is the input for the next iteration\n",
    "    pattern = pattern[1:len(pattern)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the output and wirte to txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "with open(\"./output/prediction.txt\", \"w\") as f:\n",
    "    f.write(\"\".join([int_to_char[value] for value in result_output]))\n",
    "    print(\"done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
