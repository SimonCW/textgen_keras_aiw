{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.utils import np_utils\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import ASCII text of the books \"Alice in Wonderland\" (downloaded from project Gutenberg) and convert all characters to lowercase so that the model does not treat capitalized words as new words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text and convert to lowercase\n",
    "filename = \"./input/wonderland.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For local testing, I decrease the text size dramatically to the first 1000 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decrease input text size drastically for local testing\n",
    "#raw_text = raw_text[0:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that the Neural Network can use the input, each unique character is mapped to an integer value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapping of unique charst to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c,i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary Statistics of the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  1000\n",
      "Total distinct characters:  36\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", str(n_chars))\n",
    "print(\"Total distinct characters: \", str(n_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A training pattern are the firtst 100 characters, the ground-truth \"label\" for this first training pattern then is the 101st character. This window of 100 characters then gets slided character by character.\n",
    "\n",
    "For illustration purposes assume we would take a sequence of 5 characters, then the first training sample would be `chapt` -> `e` and the second training sample would be `hapte` -> `r`. \n",
    "\n",
    "The characters get converted to integers using the lookup dictionary created before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # of Patterns: 900\n"
     ]
    }
   ],
   "source": [
    "# Prepare dataset\n",
    "seq_length = 100\n",
    "trainX = []\n",
    "trainY = []\n",
    "for i in range(0, n_chars-seq_length, 1):\n",
    "    seq_in = raw_text[i : i + seq_length]   # in 1st iteration contains first 100 chars\n",
    "    seq_out = raw_text[i + seq_length]   # in 1st iteration contains 101st char\n",
    "    trainX.append([char_to_int[char] for char in seq_in])   # char is the character as string, char_to_int[char] gives the int value\n",
    "    trainX\n",
    "    trainY.append(char_to_int[seq_out])\n",
    "n_patterns = len(trainX)\n",
    "print(\"Total # of Patterns: \" + str(n_patterns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the network we need to transform the data further. \n",
    "1. Reshape the training data to the form [samples, time steps, features].\n",
    "2. Rescale the integers to the range 0-1 to better train the network if a sigmoid function is used (stay in approx linear part of the sigmoid). \n",
    "3. One-hot encode trainY so that each character in y is represented by a vector of 45 (number of distinct characters) values. The character \"n\" (inter value 32) is then represented by a vector of zeros except for one \"1\" in column 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to [samples, time steps, features]\n",
    "X = np.reshape(trainX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one-hot encode y\n",
    "y = np_utils.to_categorical(trainY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I define a LSTM model with two hidden layers and 256 memory units. Dropout is used with a probability of 20%. The output layer is a Dense layer using the softmax activation function. This outputs a probability prediction for each character. These are the parameters used in the tutorial. They are not very well tuned but merely a starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape = (X.shape[1], X.shape[2]), return_sequences = True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation = \"softmax\"))\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I save the model weights after each epoch so that I can use the weights that produced the model with the lowest error for prediction afterwards. I also save the tensorboard callbacks to be able to see "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define checkpoints\n",
    "filepath = \"./checkpoints/weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor = \"loss\", verbose = 1, save_best_only = True, mode = \"min\")\n",
    "tensorboard_cb = TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=32, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "callbacks_list = [checkpoint, tensorboard_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 3.2723Epoch 00001: loss improved from inf to 3.27856, saving model to ./checkpoints/weights-improvement-01-3.2786.hdf5\n",
      "900/900 [==============================] - 19s 21ms/step - loss: 3.2786\n",
      "Epoch 2/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 3.0248Epoch 00002: loss improved from 3.27856 to 3.02494, saving model to ./checkpoints/weights-improvement-02-3.0249.hdf5\n",
      "900/900 [==============================] - 18s 20ms/step - loss: 3.0249\n",
      "Epoch 3/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 3.0213Epoch 00003: loss improved from 3.02494 to 3.02013, saving model to ./checkpoints/weights-improvement-03-3.0201.hdf5\n",
      "900/900 [==============================] - 21s 23ms/step - loss: 3.0201\n",
      "Epoch 4/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 3.0091Epoch 00004: loss improved from 3.02013 to 3.01342, saving model to ./checkpoints/weights-improvement-04-3.0134.hdf5\n",
      "900/900 [==============================] - 21s 23ms/step - loss: 3.0134\n",
      "Epoch 5/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 2.9984Epoch 00005: loss improved from 3.01342 to 2.99798, saving model to ./checkpoints/weights-improvement-05-2.9980.hdf5\n",
      "900/900 [==============================] - 22s 24ms/step - loss: 2.9980\n",
      "Epoch 6/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 2.9896Epoch 00006: loss improved from 2.99798 to 2.99189, saving model to ./checkpoints/weights-improvement-06-2.9919.hdf5\n",
      "900/900 [==============================] - 21s 24ms/step - loss: 2.9919\n",
      "Epoch 7/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 2.9825Epoch 00007: loss improved from 2.99189 to 2.98343, saving model to ./checkpoints/weights-improvement-07-2.9834.hdf5\n",
      "900/900 [==============================] - 21s 24ms/step - loss: 2.9834\n",
      "Epoch 8/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 2.9849Epoch 00008: loss did not improve\n",
      "900/900 [==============================] - 22s 25ms/step - loss: 2.9845\n",
      "Epoch 9/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 2.9692Epoch 00009: loss improved from 2.98343 to 2.97228, saving model to ./checkpoints/weights-improvement-09-2.9723.hdf5\n",
      "900/900 [==============================] - 23s 25ms/step - loss: 2.9723\n",
      "Epoch 10/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 2.9722Epoch 00010: loss did not improve\n",
      "900/900 [==============================] - 25s 27ms/step - loss: 2.9723\n",
      "Epoch 11/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 2.9774Epoch 00011: loss did not improve\n",
      "900/900 [==============================] - 24s 27ms/step - loss: 2.9786\n",
      "Epoch 12/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 2.9682Epoch 00012: loss improved from 2.97228 to 2.96784, saving model to ./checkpoints/weights-improvement-12-2.9678.hdf5\n",
      "900/900 [==============================] - 21s 24ms/step - loss: 2.9678\n",
      "Epoch 13/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 2.9864Epoch 00013: loss did not improve\n",
      "900/900 [==============================] - 24s 27ms/step - loss: 2.9830\n",
      "Epoch 14/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 2.9892Epoch 00014: loss did not improve\n",
      "900/900 [==============================] - 22s 25ms/step - loss: 2.9869\n",
      "Epoch 15/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 2.9736Epoch 00015: loss did not improve\n",
      "900/900 [==============================] - 21s 24ms/step - loss: 2.9712\n",
      "Epoch 16/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 2.9734Epoch 00016: loss did not improve\n",
      "900/900 [==============================] - 23s 25ms/step - loss: 2.9746\n",
      "Epoch 17/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 2.9647Epoch 00017: loss improved from 2.96784 to 2.96306, saving model to ./checkpoints/weights-improvement-17-2.9631.hdf5\n",
      "900/900 [==============================] - 23s 26ms/step - loss: 2.9631\n",
      "Epoch 18/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 2.9741Epoch 00018: loss did not improve\n",
      "900/900 [==============================] - 30s 33ms/step - loss: 2.9768\n",
      "Epoch 19/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 2.9742Epoch 00019: loss did not improve\n",
      "900/900 [==============================] - 29s 32ms/step - loss: 2.9747\n",
      "Epoch 20/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 2.9692Epoch 00020: loss did not improve\n",
      "900/900 [==============================] - 23s 26ms/step - loss: 2.9682\n",
      "Epoch 21/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 2.9641Epoch 00021: loss did not improve\n",
      "900/900 [==============================] - 24s 27ms/step - loss: 2.9659\n",
      "Epoch 22/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 2.9585Epoch 00022: loss improved from 2.96306 to 2.96072, saving model to ./checkpoints/weights-improvement-22-2.9607.hdf5\n",
      "900/900 [==============================] - 23s 26ms/step - loss: 2.9607\n",
      "Epoch 23/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 2.9596Epoch 00023: loss improved from 2.96072 to 2.95885, saving model to ./checkpoints/weights-improvement-23-2.9589.hdf5\n",
      "900/900 [==============================] - 24s 27ms/step - loss: 2.9589\n",
      "Epoch 24/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 2.9639Epoch 00024: loss did not improve\n",
      "900/900 [==============================] - 27s 30ms/step - loss: 2.9610\n",
      "Epoch 25/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 2.9537Epoch 00025: loss improved from 2.95885 to 2.95466, saving model to ./checkpoints/weights-improvement-25-2.9547.hdf5\n",
      "900/900 [==============================] - 25s 28ms/step - loss: 2.9547\n",
      "Epoch 26/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 2.9484Epoch 00026: loss improved from 2.95466 to 2.94815, saving model to ./checkpoints/weights-improvement-26-2.9482.hdf5\n",
      "900/900 [==============================] - 24s 26ms/step - loss: 2.9482\n",
      "Epoch 27/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 2.9458Epoch 00027: loss improved from 2.94815 to 2.94332, saving model to ./checkpoints/weights-improvement-27-2.9433.hdf5\n",
      "900/900 [==============================] - 27s 30ms/step - loss: 2.9433\n",
      "Epoch 28/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 2.9430Epoch 00028: loss did not improve\n",
      "900/900 [==============================] - 27s 30ms/step - loss: 2.9448\n",
      "Epoch 29/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 2.9223Epoch 00029: loss improved from 2.94332 to 2.92175, saving model to ./checkpoints/weights-improvement-29-2.9217.hdf5\n",
      "900/900 [==============================] - 22s 24ms/step - loss: 2.9217\n",
      "Epoch 30/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 2.9125Epoch 00030: loss improved from 2.92175 to 2.91212, saving model to ./checkpoints/weights-improvement-30-2.9121.hdf5\n",
      "900/900 [==============================] - 26s 29ms/step - loss: 2.9121\n",
      "Epoch 31/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 2.9049Epoch 00031: loss improved from 2.91212 to 2.90821, saving model to ./checkpoints/weights-improvement-31-2.9082.hdf5\n",
      "900/900 [==============================] - 23s 26ms/step - loss: 2.9082\n",
      "Epoch 32/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 2.9117Epoch 00032: loss did not improve\n",
      "900/900 [==============================] - 27s 30ms/step - loss: 2.9109\n",
      "Epoch 33/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 2.8937Epoch 00033: loss improved from 2.90821 to 2.89487, saving model to ./checkpoints/weights-improvement-33-2.8949.hdf5\n",
      "900/900 [==============================] - 22s 25ms/step - loss: 2.8949\n",
      "Epoch 34/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 2.8750Epoch 00034: loss improved from 2.89487 to 2.87613, saving model to ./checkpoints/weights-improvement-34-2.8761.hdf5\n",
      "900/900 [==============================] - 26s 29ms/step - loss: 2.8761\n",
      "Epoch 35/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 2.8720Epoch 00035: loss improved from 2.87613 to 2.87186, saving model to ./checkpoints/weights-improvement-35-2.8719.hdf5\n",
      "900/900 [==============================] - 23s 26ms/step - loss: 2.8719\n",
      "Epoch 36/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "896/900 [============================>.] - ETA: 0s - loss: 2.8746Epoch 00036: loss did not improve\n",
      "900/900 [==============================] - 26s 29ms/step - loss: 2.8735\n",
      "Epoch 37/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 2.8691Epoch 00037: loss improved from 2.87186 to 2.86813, saving model to ./checkpoints/weights-improvement-37-2.8681.hdf5\n",
      "900/900 [==============================] - 24s 27ms/step - loss: 2.8681\n",
      "Epoch 38/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 2.8598Epoch 00038: loss improved from 2.86813 to 2.86210, saving model to ./checkpoints/weights-improvement-38-2.8621.hdf5\n",
      "900/900 [==============================] - 24s 27ms/step - loss: 2.8621\n",
      "Epoch 39/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 2.8459Epoch 00039: loss improved from 2.86210 to 2.84423, saving model to ./checkpoints/weights-improvement-39-2.8442.hdf5\n",
      "900/900 [==============================] - 26s 29ms/step - loss: 2.8442\n",
      "Epoch 40/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 2.8360Epoch 00040: loss improved from 2.84423 to 2.83907, saving model to ./checkpoints/weights-improvement-40-2.8391.hdf5\n",
      "900/900 [==============================] - 25s 28ms/step - loss: 2.8391\n",
      "Epoch 41/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 2.8474Epoch 00041: loss did not improve\n",
      "900/900 [==============================] - 25s 28ms/step - loss: 2.8435\n",
      "Epoch 42/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 2.8311Epoch 00042: loss improved from 2.83907 to 2.82772, saving model to ./checkpoints/weights-improvement-42-2.8277.hdf5\n",
      "900/900 [==============================] - 25s 27ms/step - loss: 2.8277\n",
      "Epoch 43/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 2.8053Epoch 00043: loss improved from 2.82772 to 2.80897, saving model to ./checkpoints/weights-improvement-43-2.8090.hdf5\n",
      "900/900 [==============================] - 26s 29ms/step - loss: 2.8090\n",
      "Epoch 44/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 2.7960Epoch 00044: loss improved from 2.80897 to 2.79508, saving model to ./checkpoints/weights-improvement-44-2.7951.hdf5\n",
      "900/900 [==============================] - 26s 29ms/step - loss: 2.7951\n",
      "Epoch 45/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 2.7821Epoch 00045: loss improved from 2.79508 to 2.78479, saving model to ./checkpoints/weights-improvement-45-2.7848.hdf5\n",
      "900/900 [==============================] - 27s 30ms/step - loss: 2.7848\n",
      "Epoch 46/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 2.7726Epoch 00046: loss improved from 2.78479 to 2.77283, saving model to ./checkpoints/weights-improvement-46-2.7728.hdf5\n",
      "900/900 [==============================] - 27s 30ms/step - loss: 2.7728\n",
      "Epoch 47/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 2.7715Epoch 00047: loss improved from 2.77283 to 2.76884, saving model to ./checkpoints/weights-improvement-47-2.7688.hdf5\n",
      "900/900 [==============================] - 25s 27ms/step - loss: 2.7688\n",
      "Epoch 48/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 2.7652Epoch 00048: loss improved from 2.76884 to 2.76107, saving model to ./checkpoints/weights-improvement-48-2.7611.hdf5\n",
      "900/900 [==============================] - 24s 27ms/step - loss: 2.7611\n",
      "Epoch 49/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 2.7596Epoch 00049: loss improved from 2.76107 to 2.75737, saving model to ./checkpoints/weights-improvement-49-2.7574.hdf5\n",
      "900/900 [==============================] - 25s 27ms/step - loss: 2.7574\n",
      "Epoch 50/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 2.7298Epoch 00050: loss improved from 2.75737 to 2.72764, saving model to ./checkpoints/weights-improvement-50-2.7276.hdf5\n",
      "900/900 [==============================] - 24s 27ms/step - loss: 2.7276\n",
      "Epoch 51/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 2.7047Epoch 00051: loss improved from 2.72764 to 2.70798, saving model to ./checkpoints/weights-improvement-51-2.7080.hdf5\n",
      "900/900 [==============================] - 24s 27ms/step - loss: 2.7080\n",
      "Epoch 52/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 2.6998Epoch 00052: loss improved from 2.70798 to 2.69817, saving model to ./checkpoints/weights-improvement-52-2.6982.hdf5\n",
      "900/900 [==============================] - 27s 30ms/step - loss: 2.6982\n",
      "Epoch 53/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 2.6696Epoch 00053: loss improved from 2.69817 to 2.66850, saving model to ./checkpoints/weights-improvement-53-2.6685.hdf5\n",
      "900/900 [==============================] - 26s 29ms/step - loss: 2.6685\n",
      "Epoch 54/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 2.6694Epoch 00054: loss did not improve\n",
      "900/900 [==============================] - 27s 30ms/step - loss: 2.6709\n",
      "Epoch 55/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 2.6605Epoch 00055: loss improved from 2.66850 to 2.65851, saving model to ./checkpoints/weights-improvement-55-2.6585.hdf5\n",
      "900/900 [==============================] - 26s 29ms/step - loss: 2.6585\n",
      "Epoch 56/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 2.6259Epoch 00056: loss improved from 2.65851 to 2.62389, saving model to ./checkpoints/weights-improvement-56-2.6239.hdf5\n",
      "900/900 [==============================] - 25s 28ms/step - loss: 2.6239\n",
      "Epoch 57/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 2.6157Epoch 00057: loss improved from 2.62389 to 2.61560, saving model to ./checkpoints/weights-improvement-57-2.6156.hdf5\n",
      "900/900 [==============================] - 25s 28ms/step - loss: 2.6156\n",
      "Epoch 58/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 2.5976Epoch 00058: loss improved from 2.61560 to 2.59594, saving model to ./checkpoints/weights-improvement-58-2.5959.hdf5\n",
      "900/900 [==============================] - 25s 27ms/step - loss: 2.5959\n",
      "Epoch 59/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 2.5907Epoch 00059: loss improved from 2.59594 to 2.58912, saving model to ./checkpoints/weights-improvement-59-2.5891.hdf5\n",
      "900/900 [==============================] - 25s 28ms/step - loss: 2.5891\n",
      "Epoch 60/60\n",
      "896/900 [============================>.] - ETA: 0s - loss: 2.5655Epoch 00060: loss improved from 2.58912 to 2.56295, saving model to ./checkpoints/weights-improvement-60-2.5629.hdf5\n",
      "900/900 [==============================] - 25s 27ms/step - loss: 2.5629\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23fca2cd1d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs = 60, batch_size = 64, callbacks = callbacks_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
